{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188a7d63-aa24-405b-95eb-37125a9b9cb8",
   "metadata": {},
   "source": [
    "## Bucket Ingest Function\n",
    "\n",
    "The following is all the script that defines the \"ingest_files_from_bucket\" function. Run the code cell (shift+enter) to load the script into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43a32c7-9f6d-4d4a-95ec-fedf535457b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from FDMBuilder.FDM_helpers import *\n",
    "from google.cloud import bigquery, storage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PROJECT = \"yhcr-prd-phm-bia-core\"\n",
    "# CLIENT = bigquery.Client(project=PROJECT)\n",
    "\n",
    "def list_files_in_bucket(bucket_name):\n",
    "    bucket = storage.Client().bucket(bucket_name)\n",
    "    return [blob.name for blob in bucket.list_blobs()]\n",
    "\n",
    "\n",
    "def convert_str_camel_to_snake(str):\n",
    "    output = re.sub('([^A-Z])([A-Z][a-z]+)', r'\\1_\\2', str)\n",
    "    output = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', output)\n",
    "    output = re.sub('([A-Z]+)([A-Z])([^_0-9A-Z])', r'\\1_\\2\\3', output)\n",
    "    output = re.sub('_+', r'_', output)\n",
    "    return output.lower()\n",
    "\n",
    "\n",
    "def convert_column_names_camel_to_snake(df):\n",
    "    new_names_dict = {col: convert_str_camel_to_snake(col) \n",
    "                      for col in df.columns}\n",
    "    return df.rename(new_names_dict, axis=1)\n",
    "\n",
    "\n",
    "def remove_illegal_chars_from_str(string):\n",
    "    return re.sub(r'[^a-zA-Z0-9_]', '', string)\n",
    "\n",
    "\n",
    "def remove_num_chars_from_str_start(string):\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        if string[0].isnumeric():\n",
    "            string = string[1:] + string[0]\n",
    "        else:\n",
    "            finished = True\n",
    "    return string\n",
    "            \n",
    "\n",
    "def sanitise_column_name(col_name):\n",
    "    output = remove_illegal_chars_from_str(col_name)\n",
    "    output = remove_num_chars_from_str_start(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def sanitise_column_names(df):\n",
    "    corrected_names_dict = {col: sanitise_column_name(col)\n",
    "                            for col in df.columns}\n",
    "    return df.rename(corrected_names_dict, axis=1)\n",
    "\n",
    "\n",
    "def find_prefix_suffix(names, prop):\n",
    "    return_chars = []\n",
    "    loop_idx = 0\n",
    "    continue_search = True\n",
    "    is_prefix = None\n",
    "    while continue_search:\n",
    "        char_counts = Counter([name[loop_idx]  \n",
    "                               for name in names \n",
    "                               if len(name) > abs(loop_idx)])\n",
    "        most_common_char, n_appears = char_counts.most_common(1)[0]\n",
    "        prop_same = n_appears / char_counts.total()\n",
    "        if prop_same > prop: \n",
    "            if loop_idx < 0:\n",
    "                return_chars.insert(0, most_common_char)\n",
    "                is_prefix = False \n",
    "            else:\n",
    "                return_chars.append(most_common_char)\n",
    "                is_prefix = True \n",
    "            if loop_idx < 0:\n",
    "                loop_idx -= 1\n",
    "            else:\n",
    "                loop_idx += 1\n",
    "        elif loop_idx == 0:\n",
    "            loop_idx = -1\n",
    "        else:\n",
    "            continue_search = False\n",
    "    return \"\".join(return_chars), is_prefix\n",
    "\n",
    "\n",
    "def remove_colname_prefix_suffix(df):\n",
    "    prefix, is_prefix = find_prefix_suffix(df.columns, prop=0.95)\n",
    "    if is_prefix is None:\n",
    "        return df\n",
    "    elif is_prefix:\n",
    "        rename_dict = {col: col[len(prefix):] for col in df.columns  \n",
    "                       if col[:len(prefix)] == prefix}\n",
    "    else:\n",
    "        rename_dict = {col: col[:-len(prefix)] for col in df.columns  \n",
    "                       if col[-len(prefix):] == prefix}\n",
    "    return df.rename(rename_dict, axis=1)\n",
    "    \n",
    "    \n",
    "def percent_object_cols(df):\n",
    "    object_cols_count = sum([df[col].dtype == \"object\" \n",
    "                             for col in df.columns])\n",
    "    object_cols_pct = object_cols_count / df.shape[1] * 100\n",
    "    return object_cols_pct\n",
    "\n",
    "\n",
    "def convert_object_cols_to_str(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df[col] = df[col].astype(\"str\")\n",
    "            df[col] = df[col].replace(to_replace=\"nan\", value=None)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_date_cols(df):\n",
    "    \"\"\"\n",
    "    Infer datatype of a pandas column, process only if the column dtype is object. \n",
    "    input:   col: a pandas Series representing a df column. \n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "            except:\n",
    "                continue\n",
    "    return df\n",
    "\n",
    "\n",
    "def ingest_files_from_bucket(bucket_id, file_names, destination_dataset, \n",
    "                             separators=None, convert_colnames=False,\n",
    "                             encoding=\"utf_8\", skiprows=None,\n",
    "                             skipfooter=0):\n",
    "    \n",
    "    for i, filename in enumerate(file_names):\n",
    "        print(f\"Uploading file {i+1} of {len(file_names)}: {filename}\")\n",
    "        hyperparams = []\n",
    "        hyperparam_options = [separators, encoding, skiprows, skipfooter]\n",
    "        for option in hyperparam_options:\n",
    "            if type(option) == dict:\n",
    "                hyperparams.append(option[filename])\n",
    "            else: \n",
    "                hyperparams.append(option)\n",
    "                \n",
    "        sep, encoding, skiprows, skpfooter = hyperparams\n",
    "\n",
    "        try:\n",
    "            table_df = pd.read_csv(f\"gcs://{bucket_id}/{filename}\",  \n",
    "                                   engine=\"python\",\n",
    "                                   encoding=encoding,\n",
    "                                   sep=sep,\n",
    "                                   skiprows=skiprows,\n",
    "                                   skipfooter=skipfooter)\n",
    "        except Exception as e:\n",
    "            print(f\"\\tWarning: Skipping {filename} - attempting to read csv resulted in \"\n",
    "                  f\"the following error:\\n\\t  {e}\")\n",
    "            continue\n",
    "        if table_df.shape[0] == 0:\n",
    "            print(f\"\\tWarning: Skipping {filename} - csv is an empty table\")\n",
    "            continue\n",
    "        elif table_df.shape[1] == 1:\n",
    "            print(f\"\\tWarning: Skipping {filename} - the table parsed with only one \"\n",
    "                  \"column\")\n",
    "            continue\n",
    "        elif percent_object_cols(table_df) > 90:\n",
    "            print(f\"\\tWarning: Looks like there may be an issue with {filename}:\\n\"\n",
    "                  \"> 90% of the columns were parsed as STRING\")\n",
    "            \n",
    "        table_df = (table_df\n",
    "                    .pipe(convert_date_cols) \n",
    "                    .pipe(sanitise_column_names)\n",
    "                    .pipe(convert_object_cols_to_str))\n",
    "        \n",
    "        if convert_colnames:\n",
    "            table_df = (table_df\n",
    "                        .pipe(convert_column_names_camel_to_snake)\n",
    "                        .pipe(remove_colname_prefix_suffix))\n",
    "            \n",
    "        if \".\" in filename:\n",
    "            name = remove_illegal_chars_from_str(filename.split(\".\")[0])\n",
    "        else:\n",
    "            name = remove_illegal_chars_from_str(filename)\n",
    "        table_df.to_gbq(f\"{PROJECT}.{destination_dataset}.{name}\",\n",
    "                  project_id=PROJECT,\n",
    "                  if_exists=\"fail\",\n",
    "                  progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f1fc4b-0a73-44b3-89c2-0f089c179198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/yhcr-prd-phm-bia-core/jobs?prettyPrint=false: Access Denied: Project yhcr-prd-phm-bia-core: User does not have bigquery.jobs.create permission in project yhcr-prd-phm-bia-core.\n",
      "\n",
      "Location: None\n",
      "Job ID: 1005e5f6-658a-4796-b2f1-1f2eaac2efcd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM `yhcr-prd-phm-bia-core.CY_FDM_ChildrensSocialCare.CPP` LIMIT 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fc156-c08c-45c5-adc2-76ebb19eb534",
   "metadata": {},
   "source": [
    "## Usage:\n",
    "\n",
    "The \"ingest_files_from_bucket\" function takes the following arguments:\n",
    "\n",
    "* **bucket_id**: a string that names the bucket where the files are currently stored - this can include a directory inside a bucket if needed i.e. \"bucket-name/directory-name\"\n",
    "* **file_names**: a list of file names within the bucket/directory that are to be uploaded. Lists are defined by square brackets i.e. file_names = [\"file1.csv\", \"file2.csv\", \"file3.csv\"]\n",
    "* **destination_dataset**: a string naming the dataset id where the files are to be ingested\n",
    "* **convert_colnames**: True/False (default=False) The function includes an optional feature that converts column names *FromCamelCase* *to_snake_case* \n",
    "\n",
    "\\*note: all column names will have non-alphanumeric characters removed, and any column names that begin with numbers will have the numbers moved from the start to the end of the name. This ensures the column names agree with the accepted format in bigquery, otherwise bigquery would throw an error.\n",
    "    \n",
    "* **separators**: the separator used in the file - i.e. a standard csv would have separator=\",\" **Note:** A PIPE IS A SPECIAL CHARACTER AND MUST BE ESCAPED \"\\\\|\". Likewise, tab is a special character and is written \"\\t\"\n",
    "* **skprows**: a list containing the row numbers of any rows in the file to be skipped **Note:** Python is 0 indexed! The first row is zero.\n",
    "* **encoding**: the file's character encoding - this tends to be either \"utf_8\" or \"utf_16\".\n",
    "* **skipfooter**: an integer number of rows to skip at the end of the file - note, this is not an indexed function like \"skiprows\" and so can only be used to remove a chunk of rows at the end of a file. Removing files at specific indeces must be done with \"skiprows\"\n",
    "\n",
    "\\*note: the above three arguments can be specified as individual values that will be used for every file i.e. separators = \",\" or can be dictionaries with the keys as filenames and the values as the argument i.e separators={\"file1.csv\":\",\", \"file2.csv\": \"\\\\|\", ...}\n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "**CAREFULLY READ THE CONSOLE OUTPUT WHEN THE FUNCTION IS RUNNING**\n",
    "\n",
    "Given the range of potential file formats, and even the subset of those I've already seen, it simply isn't possible to automatically account for every possible situation. The function is designed to simply read any errors to the console and move on to the next file if it can't upload for any reason. It's also possible that odd edge cases will throw a python error and stop execution all-together.\n",
    "\n",
    "Really, the only viable solution to having a simple \"fire and forget\" upload function is to ensure that all the upload files are in the same format **before** being uploaded.\n",
    "\n",
    "The following cell contains an example usage of the ingest function as a reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95c2d05-7f34-4750-baf3-fc0902df3541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file 1 of 1: attendance.csv\n"
     ]
    }
   ],
   "source": [
    "bucket_id = \"yhcr-prd-phm-bia-core-data-landing-bradford-tmp\"\n",
    "file_names = [\"attendance.csv\"]\n",
    "destination_dataset = \"CY_CLASS_ACT\"\n",
    "ingest_files_from_bucket(bucket_id=bucket_id, \n",
    "                         file_names=file_names, \n",
    "                         destination_dataset=destination_dataset,\n",
    "                         encoding=\"utf_8\",\n",
    "                         convert_colnames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "056282a9-384c-43b7-bd4c-fb8aaaadf9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df.to_gbq(f\"{PROJECT}.CY_CLASS_ACT.attendance\",  \n",
    "                project_id=PROJECT,  \n",
    "                if_exists=\"fail\",  \n",
    "                progress_bar=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-FDMEnv-py",
   "name": "r-cpu.4-1.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m90"
  },
  "kernelspec": {
   "display_name": "Python [conda env:FDMEnv]",
   "language": "python",
   "name": "conda-env-FDMEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
